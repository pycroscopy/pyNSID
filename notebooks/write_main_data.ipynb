{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dimensions(dimensions, dim_type='Position'):\n",
    "    \"\"\"\n",
    "    Checks if the provided object is an iterable with pyUSID.Dimension objects.\n",
    "    If it is not full of Dimension objects, Exceptions are raised.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dimensions : iterable or pyUSID.Dimension\n",
    "        Iterable containing pyUSID.Dimension objects\n",
    "    dim_type : str, Optional. Default = \"Position\"\n",
    "        Type of Dimensions in the iterable. Set to \"Spectroscopic\" if not Position dimensions.\n",
    "        This string is only used for more descriptive Exceptions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List containing pyUSID.Dimension objects\n",
    "    \"\"\"\n",
    "    if isinstance(dimensions, Dimension):\n",
    "        dimensions = [dimensions]\n",
    "    if isinstance(dimensions, np.ndarray):\n",
    "        if dimensions.ndim > 1:\n",
    "            dimensions = dimensions.ravel()\n",
    "            warn(dim_type + ' dimensions should be specified by a 1D array-like. Raveled this numpy array for now')\n",
    "    if not isinstance(dimensions, (list, np.ndarray, tuple)):\n",
    "        raise TypeError(dim_type + ' dimensions should be array-like of Dimension objects')\n",
    "    if not np.all([isinstance(x, Dimension) for x in dimensions]):\n",
    "        raise TypeError(dim_type + ' dimensions should be a sequence of Dimension objects')\n",
    "    return dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode = str\n",
    "class Dimension(object):\n",
    "    \"\"\"\n",
    "    ..autoclass::Dimension\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, quantity, units, values, is_position):\n",
    "        \"\"\"\n",
    "        Simple object that describes a dimension in a dataset by its name, units, and values\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str or unicode\n",
    "            Name of the dimension. For example 'X'\n",
    "        quantity : str or unicode\n",
    "            Quantity for this dimension. For example: 'Length'\n",
    "        units : str or unicode\n",
    "            Units for this dimension. For example: 'um'\n",
    "        values : array-like or int\n",
    "            Values over which this dimension was varied. A linearly increasing set of values will be generated if an\n",
    "            integer is provided instead of an array.\n",
    "        is_position : bool\n",
    "            Whether or not this is a position or spectroscopy dimensions\n",
    "        \"\"\"\n",
    "        #name = validate_single_string_arg(name, 'name')\n",
    "        #quantity = validate_single_string_arg(quantity, 'quantity')\n",
    "\n",
    "        if not isinstance(units, (str, unicode)):\n",
    "            raise TypeError('units should be a string')\n",
    "        units = units.strip()\n",
    "\n",
    "        if isinstance(values, int):\n",
    "            if values < 1:\n",
    "                raise ValueError('values should at least be specified as a positive integer')\n",
    "            values = np.arange(values)\n",
    "        if not isinstance(values, (np.ndarray, list, tuple)):\n",
    "            raise TypeError('values should be array-like')\n",
    "        values = np.array(values)\n",
    "        if values.ndim > 1:\n",
    "            raise ValueError('Values for dimension: {} are not 1-dimensional'.format(name))\n",
    "\n",
    "        if not isinstance(is_position, bool):\n",
    "            raise TypeError('is_position should be a bool')\n",
    "\n",
    "        self.name = name\n",
    "        self.quantity = quantity\n",
    "        self.units = units\n",
    "        self.values = values\n",
    "        self.is_position = is_position\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{} - {} ({}): {}'.format(self.name, self.quantity, self.units, self.values)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Dimension):\n",
    "            if self.name != other.name:\n",
    "                return False\n",
    "            if self.units != other.units:\n",
    "                return False\n",
    "            if self.quantity != other.quantity:\n",
    "                return False\n",
    "            if len(self.values) != len(other.values):\n",
    "                return False\n",
    "            if not np.allclose(self.values, other.values):\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: Y - Length (um): [ 0.   2.5  5.   7.5 10. ],\n",
       " 1: X - Length (um): [0. 1. 2. 3. 4. 5. 6.],\n",
       " 2: DC offset - Bias (V): [ 0.00000000e+00  5.87785252e-01  9.51056516e-01  9.51056516e-01\n",
       "   5.87785252e-01  1.22464680e-16 -5.87785252e-01 -9.51056516e-01\n",
       "  -9.51056516e-01 -5.87785252e-01 -2.44929360e-16],\n",
       " 3: BE Frequency - Frequency (Hz): [ 0.  5. 10.]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_name  = 'nDim_Data'\n",
    "main_data = np.random.rand(5, 7, 11, 3)\n",
    "quantity = 'intensity'\n",
    "units=\"pixel\"\n",
    "\n",
    "\n",
    "dim_dict = {0: Dimension('Y', 'Length', 'um', np.linspace(0, 10, num=5), True),\n",
    "        1: Dimension('X', 'Length', 'um', np.linspace(0, 6, num=7), True),\n",
    "        2: Dimension('DC offset', 'Bias', 'V', np.sin(np.linspace(0, 1, num=11) * 2 * np.pi), True),\n",
    "        3: Dimension('BE Frequency', 'Frequency', 'Hz', np.linspace(0, 10, num=3), True)}\n",
    "\n",
    "dim_dict \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make pyhdf5 file and channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: Y - Length (um): [ 0.   2.5  5.   7.5 10. ],\n",
       " 1: X - Length (um): [0. 1. 2. 3. 4. 5. 6.],\n",
       " 2: <HDF5 dataset \"DC offset\": shape (11,), type \"<f8\">,\n",
       " 3: BE Frequency - Frequency (Hz): [ 0.  5. 10.]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    h5_file.close()\n",
    "except:\n",
    "    pass\n",
    "h5_file =  h5py.File('test.hf5', mode='a')\n",
    "if \"Measurement_000/Channel_000\" in h5_file:\n",
    "    current_channel = h5_file[\"Measurement_000/Channel_000\"]\n",
    "else:\n",
    "    current_channel = h5_file.create_group(\"Measurement_000/Channel_000\")\n",
    "\n",
    "if 'DC offset' not in current_channel:\n",
    "    current_channel['DC offset'] = np.sin(np.linspace(0, 1, num=11) * 2 * np.pi)\n",
    "\n",
    "dim_dict[2] = current_channel['DC offset'] \n",
    "current_channel['DC offset'].attrs['name']= 'DC offset'\n",
    "current_channel['DC offset'].attrs['quantity']= 'Bias'\n",
    "current_channel['DC offset'].attrs['units'] = 'V'\n",
    "current_channel['DC offset'].attrs['is_position'] = False\n",
    "dim_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activate pyUSID helper functions\n",
    "\n",
    "We can see which functions can be unchanged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyUSID as usid\n",
    "from pyUSID.io.hdf_utils.base import get_attr, write_simple_attrs, is_editable_h5, write_book_keeping_attrs\n",
    "#from pyUSID.io.hdf_utils.simple import link_as_main, check_if_main, write_ind_val_dsets, validate_dims_against_main, validate_anc_h5_dsets, copy_dataset\n",
    "from pyUSID.io.dtype_utils import contains_integers, validate_dtype, validate_single_string_arg, validate_string_args, \\\n",
    "    validate_list_of_strings, lazy_load_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Validate Dimension Function \n",
    "\n",
    "notice the plural in name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dimension_dataset(this_dim,dim_shape):\n",
    "    error_message = ''\n",
    "    # Is it 1D?\n",
    "    if len(this_dim.shape)!=1:\n",
    "        error_message += ' High dimensional datasets are not allowed as dimensions;\\n'\n",
    "    # Does this dataset have a \"simple\" dtype - no compound data type allowed!\n",
    "    # is the shape matching with the main dataset?\n",
    "    if len(this_dim) != dim_shape:\n",
    "        error_message += ' Dimension has wrong length;\\n'\n",
    "    # Does it contain some ancillary attributes like 'name', quantity', 'units', and 'is_position' \n",
    "    necessary_attributes =  ['name', 'quantity', 'units', 'is_position']\n",
    "    for key in necessary_attributes:\n",
    "        if key not in this_dim.attrs:\n",
    "            error_message += f'Missing {key} attribute in dimension;\\n ' \n",
    "        # and are these of types str, str, str, and bool respectively and not empty?\n",
    "        elif key == 'is_position':\n",
    "            if this_dim.attrs['is_position'] not in [True, False]: ## isinstance is here not working \n",
    "                error_message += f'{key} attribute in dimension should be boolean;\\n ' \n",
    "        elif not isinstance(this_dim.attrs[key], str):\n",
    "            error_message += f'{key} attribute in dimension should be string;\\n ' \n",
    "    \n",
    "    \n",
    "    return error_message\n",
    "\n",
    "def validate_dimensions(main_shape, dim_dict, h5_parent_group ):\n",
    "    # Each item could either be a Dimension object or a HDF5 dataset\n",
    "    # Collect the file within which these ancillary HDF5 objectsa are present if they are provided\n",
    "    which_h5_file = {}\n",
    "    # Also collect the names of the dimensions. We want them to be unique\n",
    "    dim_names = []\n",
    "    \n",
    "    dimensions_correct = []\n",
    "    for index, dim_exp_size in enumerate(main_shape):\n",
    "        this_dim = dim_dict[index]\n",
    "        if isinstance(this_dim, h5py.Dataset):\n",
    "            print(f'{index} is a dataset')\n",
    "            error_message = check_dimension_dataset(this_dim, main_shape[index])\n",
    "                \n",
    "            # All these checks should live in a helper function for cleaniness\n",
    "            # Is it 1D?\n",
    "            # Does this dataset have a \"simple\" dtype - no compound data type allowed!\n",
    "            # is the shape matching with the main dataset?\n",
    "            # Does it contain some ancillary attributes like 'name', quantity', 'units', and 'is_position' \n",
    "            # and are these of types str, str, str, and bool respectively and not empty?\n",
    "            if len(error_message)>0:\n",
    "                print(f'Dimension {index} has the following error_message:\\n', error_message)\n",
    "            \n",
    "            else:\n",
    "                print(\"dataset ok\")\n",
    "                dim_names.append(this_dim.name)\n",
    "                # are all datasets in the same file?\n",
    "                which_h5_file[index]=this_dim.file.filename #better to keep it indictionary to keep track of index\n",
    "                \n",
    "        elif isinstance(this_dim, Dimension):\n",
    "            print('Dimension')\n",
    "            print(len(this_dim.values))\n",
    "            # is the shape matching with the main dataset?\n",
    "            dimensions_correct.append(len(this_dim.values) == dim_exp_size)\n",
    "            # Is there a HDF5 dataset with the same name already in the provided group where this dataset will be created?\n",
    "            if  this_dim.name in h5_parent_group:\n",
    "                # check if this object with the same name is a dataset and if it satisfies the above tests\n",
    "                if isinstance(h5_parent_group[this_dim.name], h5py.Dataset):\n",
    "                    print('needs more checking')\n",
    "                    # Gerd for the moment disableddimensions_correct[-1] = False\n",
    "                else:\n",
    "                    dimensions_correct[-1] = False\n",
    "            # Otherwise, just append the dimension name for the uniqueness test\n",
    "            elif this_dim.name not in dim_names:\n",
    "                dim_names.append(this_dim.name)\n",
    "            else:\n",
    "                dimensions_correct[-1] = False\n",
    "        else:\n",
    "            raise TypeError(f'Values of dim_dict should either be h5py.Dataset objects or Dimension. '\n",
    "                            'Object at index: {index} was of type: {index}')\n",
    "        \n",
    "        for dim in which_h5_file:\n",
    "            if which_h5_file[dim] != h5_parent_group.file.filename:\n",
    "                print('need to copy dimension', dim)\n",
    "        for i, dim_name in enumerate(dim_names[:-1]):\n",
    "            if dim_name in  dim_names[i+1:]:\n",
    "                print(dim_name, ' is not unique')\n",
    "    \n",
    "    return dimensions_correct \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write_main_dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def write_main_dataset(h5_parent_group, main_data, main_data_name, quantity, units, dim_dict,\n",
    "                       main_dset_attrs=None, h5_pos_inds=None, h5_pos_vals=None, h5_spec_inds=None, h5_spec_vals=None,\n",
    "                       aux_spec_prefix='Spectroscopic_', aux_pos_prefix='Position_', verbose=False,\n",
    "                       slow_to_fast=False, **kwargs):\n",
    "    \n",
    "    if not isinstance(h5_parent_group, (h5py.Group, h5py.File)):\n",
    "        raise TypeError('h5_parent_group should be a h5py.File or h5py.Group object')\n",
    "    if not is_editable_h5(h5_parent_group):\n",
    "        raise ValueError('The provided file is not editable')\n",
    "    if verbose:\n",
    "        print('h5 group and file OK')\n",
    "\n",
    "        \n",
    "    #####################\n",
    "    # Validate Main Data\n",
    "    #####################\n",
    "    quantity, units, main_data_name = validate_string_args([quantity, units, main_data_name],\n",
    "                                                           ['quantity', 'units', 'main_data_name'])\n",
    "\n",
    "    if verbose:\n",
    "            print('quantity, units, main_data_name all OK')\n",
    "\n",
    "    quantity = quantity.strip()\n",
    "    units = units.strip()\n",
    "    main_data_name = main_data_name.strip()\n",
    "    if '-' in main_data_name:\n",
    "        warn('main_data_name should not contain the \"-\" character. Reformatted name from:{} to '\n",
    "             '{}'.format(main_data_name, main_data_name.replace('-', '_')))\n",
    "    main_data_name = main_data_name.replace('-', '_')\n",
    "    \n",
    "    if isinstance(main_data, (list, tuple)):\n",
    "        if not contains_integers(main_data, min_val=1):\n",
    "            raise ValueError('main_data if specified as a shape should be a list / tuple of integers >= 1')\n",
    "        if len(main_data) < 1:\n",
    "            raise ValueError('main_data if specified as a shape should contain at least 1 number for the singular dimension')\n",
    "        if 'dtype' not in kwargs:\n",
    "            raise ValueError('dtype must be included as a kwarg when creating an empty dataset')\n",
    "        _ = validate_dtype(kwargs.get('dtype'))\n",
    "        main_shape = main_data\n",
    "        if verbose:\n",
    "            print('Selected empty dataset creation. OK so far')\n",
    "    elif isinstance(main_data, (np.ndarray, da.core.Array)):\n",
    "        main_shape = main_data.shape\n",
    "        if verbose:\n",
    "            print('Provided numpy or Dask array for main_data OK so far')\n",
    "    else:\n",
    "        raise TypeError('main_data should either be a numpy array or a tuple / list with the shape of the data')\n",
    "        \n",
    "    ######################\n",
    "    # Validate Dimensions\n",
    "    ######################\n",
    "    print( len(dim_dict) , len(main_shape))\n",
    "    # An N dimensional dataset should have N items in the dimension dictionary\n",
    "    if len(dim_dict) != len(main_shape):\n",
    "        raise ValueError('Incorrect number of dimensions: {} provided to support main data, of shape: {}'.format(len(dim_dict), main_shape))\n",
    "    if set(range(len(main_shape))) != set(dim_dict.keys()):\n",
    "        raise KeyError('')\n",
    "        \n",
    "        \n",
    "    if False in validate_dimensions(main_shape,dim_dict, h5_parent_group):\n",
    "        print('dimensions incorrect')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    print('dimensions correct')\n",
    "    \n",
    "    if main_data_name in h5_parent_group:\n",
    "        print('Oops, dataset exits')\n",
    "        #del h5_parent_group[main_data_name]\n",
    "        return\n",
    "    \n",
    "    #####################\n",
    "    # Write Main Dataset\n",
    "    ####################\n",
    "    if h5_parent_group.file.driver == 'mpio':\n",
    "        if kwargs.pop('compression', None) is not None:\n",
    "            warn('This HDF5 file has been opened with the \"mpio\" communicator. '\n",
    "                 'mpi4py does not allow creation of compressed datasets. Compression kwarg has been removed')\n",
    "    if isinstance(main_data, np.ndarray):\n",
    "        # Case 1 - simple small dataset\n",
    "        \n",
    "        h5_main = h5_parent_group.create_dataset(main_data_name, data=main_data, **kwargs)\n",
    "        if verbose:\n",
    "            print('Created main dataset with provided data')\n",
    "    elif isinstance(main_data, da.core.Array):\n",
    "        # Case 2 - Dask dataset\n",
    "        # step 0 - get rid of any automated dtype specification:\n",
    "        _ = kwargs.pop('dtype', None)\n",
    "        # step 1 - create the empty dataset:\n",
    "        h5_main = h5_parent_group.create_dataset(main_data_name, shape=main_data.shape, dtype=main_data.dtype,\n",
    "                                                 **kwargs)\n",
    "        if verbose:\n",
    "            print('Created empty dataset: {} for writing Dask dataset: {}'.format(h5_main, main_data))\n",
    "            print('Dask array will be written to HDF5 dataset: \"{}\" in file: \"{}\"'.format(h5_main.name,\n",
    "                                                                                          h5_main.file.filename))\n",
    "        # Step 2 - now ask Dask to dump data to disk\n",
    "        da.to_hdf5(h5_main.file.filename, {h5_main.name: main_data})\n",
    "        # main_data.to_hdf5(h5_main.file.filename, h5_main.name)  # Does not work with python 2 for some reason\n",
    "    else:\n",
    "        # Case 3 - large empty dataset\n",
    "        h5_main = h5_parent_group.create_dataset(main_data_name, main_data, **kwargs)\n",
    "        if verbose:\n",
    "            print('Created empty dataset for Main')    \n",
    "    \n",
    "    #################\n",
    "    # Add Dimensions\n",
    "    #################\n",
    "    for i, this_dim in dim_dict.items():\n",
    "        if isinstance(this_dim, h5py.Dataset):\n",
    "            this_dim_dset = this_dim\n",
    "        elif isinstance(this_dim, Dimension):\n",
    "            this_dim_dset = h5_parent_group.create_dataset(this_dim.name,data=this_dim.values)\n",
    "            this_dim_dset.attrs['units'] = this_dim.units\n",
    "            this_dim_dset.attrs['name'] = this_dim.name\n",
    "            this_dim_dset.attrs['quantity'] =  this_dim.quantity\n",
    "            this_dim_dset.attrs['is_position'] = this_dim.is_position\n",
    "        else:\n",
    "            print(i,' not a good dimension')\n",
    "            \n",
    "            pass\n",
    "        this_dim_dset.make_scale(this_dim_dset.attrs['name'])\n",
    "        h5_main.dims[int(i)].label = this_dim_dset.attrs['name']\n",
    "        h5_main.dims[int(i)].attach_scale(this_dim_dset)\n",
    "        \n",
    "    write_simple_attrs(h5_main, {'quantity': quantity, 'units': units})\n",
    "    if verbose:\n",
    "        print('Wrote quantity and units attributes to main dataset')\n",
    "\n",
    "    if isinstance(main_dset_attrs, dict):\n",
    "        write_simple_attrs(h5_main, main_dset_attrs)\n",
    "        if verbose:\n",
    "            print('Wrote provided attributes to main dataset')\n",
    "\n",
    "    write_book_keeping_attrs(h5_main)\n",
    "        \n",
    "    return h5_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: Y - Length (um): [ 0.   2.5  5.   7.5 10. ], 1: X - Length (um): [0. 1. 2. 3. 4. 5. 6.], 2: <HDF5 dataset \"DC offset\": shape (11,), type \"<f8\">, 3: BE Frequency - Frequency (Hz): [ 0.  5. 10.]}\n",
      "4 4\n",
      "Dimension\n",
      "5\n",
      "Dimension\n",
      "7\n",
      "2 is a dataset\n",
      "dataset ok\n",
      "Dimension\n",
      "3\n",
      "dimensions correct\n"
     ]
    }
   ],
   "source": [
    "main_data_name  = 'nDim_Data'\n",
    "main_data = np.random.rand(5, 7, 11, 3)\n",
    "quantity = 'intensity'\n",
    "units=\"pixel\"\n",
    "data_type = 'STM_spectroscopy'\n",
    "\n",
    "print(dim_dict)\n",
    "current_dataset = write_main_dataset(current_channel, main_data, main_data_name, quantity, units, dim_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Y', 'X', 'DC offset', 'BE Frequency']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(current_dataset.dims[0].keys())\n",
    "[dim.label for dim in current_dataset.dims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empty h5py Group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['DC offset']>\n"
     ]
    }
   ],
   "source": [
    "for key in current_channel:\n",
    "    if 'DC' not in key:\n",
    "        del current_channel[key]\n",
    "print(current_channel.keys())\n",
    "\n",
    "h5_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure the names are all unqiue\n",
    "\n",
    "# Check to make sure that all ancillary datasets are in the same HDF5 file using which_h5_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_dims = []\n",
    "# Now write Dimension objects to HDF5 datasets. \n",
    "for index in range(len(main_shape)):\n",
    "    this_dim = dim_dict[index]\n",
    "    if isinstance(this_dim, h5py.Dataset):\n",
    "        h5_dims.append(this_dim)\n",
    "    else: # We know by now that this is the Dimension object\n",
    "        # Write this dimension object to HDF5 dataset\n",
    "        h5_anc_dset = None\n",
    "        # Append this dataset to the list\n",
    "        h5_dims.append(h5_anc_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, we have all the ancillary datasets\n",
    "# We write the main dataset now\n",
    "\n",
    "if h5_parent_group.file.driver == 'mpio':\n",
    "    if kwargs.pop('compression', None) is not None:\n",
    "        warn('This HDF5 file has been opened with the \"mpio\" communicator. '\n",
    "             'mpi4py does not allow creation of compressed datasets. Compression kwarg has been removed')\n",
    "\n",
    "if isinstance(main_data, np.ndarray):\n",
    "    # Case 1 - simple small dataset\n",
    "    h5_main = h5_parent_group.create_dataset(main_data_name, data=main_data, **kwargs)\n",
    "    if verbose:\n",
    "        print('Created main dataset with provided data')\n",
    "elif isinstance(main_data, da.core.Array):\n",
    "    # Case 2 - Dask dataset\n",
    "    # step 0 - get rid of any automated dtype specification:\n",
    "    _ = kwargs.pop('dtype', None)\n",
    "    # step 1 - create the empty dataset:\n",
    "    h5_main = h5_parent_group.create_dataset(main_data_name, shape=main_data.shape, dtype=main_data.dtype,\n",
    "                                             **kwargs)\n",
    "    if verbose:\n",
    "        print('Created empty dataset: {} for writing Dask dataset: {}'.format(h5_main, main_data))\n",
    "        print('Dask array will be written to HDF5 dataset: \"{}\" in file: \"{}\"'.format(h5_main.name,\n",
    "                                                                                      h5_main.file.filename))\n",
    "    # Step 2 - now ask Dask to dump data to disk\n",
    "    da.to_hdf5(h5_main.file.filename, {h5_main.name: main_data})\n",
    "    # main_data.to_hdf5(h5_main.file.filename, h5_main.name)  # Does not work with python 2 for some reason\n",
    "else:\n",
    "    # Case 3 - large empty dataset\n",
    "    h5_main = h5_parent_group.create_dataset(main_data_name, main_data, **kwargs)\n",
    "    if verbose:\n",
    "        print('Created empty dataset for Main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_simple_attrs(h5_main, {'quantity': quantity, 'units': units})\n",
    "if verbose:\n",
    "    print('Wrote quantity and units attributes to main dataset')\n",
    "    \n",
    "if isinstance(main_dset_attrs, dict):\n",
    "    write_simple_attrs(h5_main, main_dset_attrs)\n",
    "    if verbose:\n",
    "        print('Wrote provided attributes to main dataset')\n",
    "\n",
    "write_book_keeping_attrs(h5_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and attach dimension scales for each ancillary dataset:\n",
    "\n",
    "if verbose:\n",
    "    print('Successfully linked datasets - dataset should be main now')\n",
    "\n",
    "for index, h5_dim in enumerate(h5_dims):\n",
    "    dim_name = get_attr(h5_dim, 'name')\n",
    "    # First make this HDF5 dataset a dimension scale\n",
    "    h5_dim.make_scale(dim_name)\n",
    "    # Attach the name of the dimension to the main dataset also\n",
    "    h5_main.dims[index].label = dim_name\n",
    "    # Finally attach the scale itself\n",
    "    h5_main.dims[index].attach_scale(h5_dim)\n",
    "    \n",
    "# Now the dataset should be a main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now return this object as a powerful object:    \n",
    "#from ..nsi_data import NSIDataset\n",
    "#return NSIDataset(h5_main)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
